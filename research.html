<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Research | Muhammad Haseeb</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>

  <header class="main-header">
    <nav class="main-nav">
      <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="research.html" class="active">Research & Projects</a></li>
        <li><a href="#publications">Publications</a></li>
        <li><a href="#projects">Projects</a></li>
        <li><a href="#experience">Experience</a></li>
        <li><a href="#opensource">Open Source</a></li>
      </ul>
    </nav>
  </header>

  <div class="container">
    <div class="page-title-container">
        <h1>Research & Projects</h1>
    </div>

    <section id="publications">
        <h2>Publications</h2>
        <div class="item" id="pub-bacp">
          <span class="item-title">BaCP: Backbone Contrastive Pruning for Preserving Representations in Extremely Sparse Neural Networks</span>
          <div class="item-meta">M. H. Khawaja, <strong>M. Haseeb</strong>, M. F. Shoaib, M. Tahir. <em>(Submitted to AAAI 2025)</em></div>
          <p class="item-description">
            As a contributor to this work, I tackled the problem of representational collapse in highly sparse networks. We designed and implemented the multi-objective loss function that aligns sparse embeddings against three critical references (pretrained, fine-tuned, and historical). We tested our framework across CNNs, Vision Transformers, and Language Models to verify that our method preserves accuracy at extreme sparsity.
          </p>
        </div>
    </section>

    <section id="projects">
        <h2>Research Projects</h2>
        <div class="item" id="project-dg">
            <span class="item-title">Domain Generalization</span>
            <div class="item-meta">Role: Project Lead | CITY, LUMS</div>
            <p class="item-description">
                Developed a framework using learnable visual queries to improve out-of-distribution performance. By training these queries with a group relative optimization objective, I guided the model to learn domain-invariant features. My implementation <strong>achieved a 3% accuracy gain</strong> on the PACS dataset over the ERM baseline. I am now extending this work by using Mechanistic Interpretability as a tool to design a methodology to physically remove domain-specific circuits from the network.
            </p>
        </div>
        <div class="item" id="project-fedmi">
            <span class="item-title">Mechanistic Analysis of Circuit Preservation in Federated Learning</span>
            <div class="item-meta">Role: Project Lead | CITY, LUMS | <a href="https://github.com/ha405/FedMI" target="_blank" rel="noopener noreferrer">GitHub</a> | <em>Preprint coming soon</em></div>
            <p class="item-description">
                I led a project investigating the internal failure modes of Federated Learning (FL) on Non-IID data through the lens of Mechanistic Interpretability. We hypothesized that performance degradation is caused by 'circuit collapse'â€”the destructive interference of functional sub-networks from conflicting client updates. By training and analyzing sparse, interpretable models in an FL simulation, we provided the first mechanistic evidence that Non-IID data causes local circuits to diverge and collapse in the global model, reframing the statistical problem of 'weight divergence' as an observable structural failure.
            </p>
        </div>
         <!-- ==== ID ADDED HERE ==== -->
        <div class="item" id="project-diffusion">
            <span class="item-title">Quantization of Diffusion Models</span>
            <div class="item-meta">Role: Project Lead | CITY & 10x Engineers, LUMS</div>
            <p class="item-description">
                To address the varying numerical precision required at different timesteps of the diffusion process, I am developing a method for timestep-aware quantization. This involves training an MLP on the signal-to-noise ratio at a timestep to predict optimal quantization parameters based on the noise schedule. My methodology beats the vanilla timestep based MLP by 55 percent.
            </p>
        </div>
        <div class="item" id="project-klawq">
            <span class="item-title">KL-Aware Quantization (KLAWQ)</span>
            <div class="item-meta">Role: Project Lead | CV & Graphics Lab, LUMS</div>
            <p class="item-description">
                Developed an augmented GPTQ framework that integrates knowledge distillation and supervised fine-tuning directly into the post-training quantization pipeline. My key technical innovation was combining the standard GPTQ Hessian with second-order curvature information from KL-divergence and cross-entropy objectives. This better preserves the teacher model's outputs, and my implementation <strong>achieved a 30% reduction in perplexity</strong> over the baseline GPTQ at equivalent bit-widths.
            </p>
        </div>
        <div class="item">
            <span class="item-title">Single-Image Camera Calibration (SOFI-UGCL)</span>
            <div class="item-meta">Role: Contributor | CV & Graphics Lab, LUMS</div>
            <p class="item-description">
                Engineered a hybrid method for camera calibration combining a Transformer front-end with geometric post-processing. I took a pretrained Multi-Scale Deformable Transformer (SOFI) to predict geometric primitives like the zenith point and horizon line. From these predictions, I <strong> recovered camera's intrinsic (K) and extrinsic (R, t) parameters</strong> by enforcing mathematical constrains over the loss function, enabling recovery of the full projection matrix from a single image.
            </p>
        </div>
    </section>

    <section id="experience">
        <h2>Experience & Teaching</h2>
        <div class="item">
            <span class="item-title">Machine Learning Engineer (Contract)</span>
            <div class="item-meta">Innova Tech</div>
            <p class="item-description">
                As an MLE, I owned the optimization pipeline for an on-device object detection system. I led ONNX and TensorRT-based quantization experiments, reducing model memory footprints by approximately 40% while maintaining real-time inference. Additionally, I automated a large-scale data annotation pipeline, which improved dataset quality and resulted in a 4% accuracy uplift in validation.
            </p>
        </div>
        <div class="item">
            <span class="item-title">Teaching Assistant, CS436: Computer Vision</span>
            <div class="item-meta">Lahore University of Management Sciences</div>
            <p class="item-description">
                As a TA for the core Computer Vision course, I designed assignments on PyTorch transfer learning and building real-time, multithreaded object detection pipelines in C++/OpenCV. I also supervised and mentored 40 student groups (80+ students) for their final course project, which involved creating a virtual tour application using Structure from Motion (SfM) techniques.
            </p>
        </div>
    </section>
    
    <section id="opensource">
        <h2>Open Source Contributions</h2>
        <div class="item">
            <span class="item-title">Contributor, pytorch-image-models (timm)</span>
            <div class="item-meta">Role: Core Evaluation Metrics</div>
            <p class="item-description">
                Implemented F1, precision, and recall metrics into the core evaluation loop of the popular `timm` library. My contribution specifically handles distributed training environments by correctly aggregating metrics across multiple GPUs using `torch.distributed` primitives, ensuring accuracy even with uneven batch sizes.
            </p>
        </div>
        <div class="item">
            <span class="item-title">Contributor, adapters (PEFT Library)</span>
            <div class="item-meta">Role: GQA Support for Modern LLMs</div>
            <p class="item-description">
                Extended the `adapters` library to support Parameter-Efficient Fine-Tuning (PEFT) for models using Group Query Attention (GQA), such as Llama-2 and Mistral. As part of this, I identified and fixed a critical tensor shape mismatch bug, unblocking PEFT for these newer, memory-efficient architectures.
            </p>
        </div>
    </section>

    <footer class="page-footer">
        <p>&copy; 2026 Muhammad Haseeb. Last Updated: January 2026.</p>
    </footer>

  </div>
</body>
</html>